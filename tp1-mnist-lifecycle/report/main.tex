\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}

% Configuration de la géométrie de la page
\geometry{hmargin=2.5cm,vmargin=2.5cm}

\title{\textbf{TP1: De la conception au déploiement de modèles de Deep Learning}}
\author{FEKE JIMMY WILSON \\ Master 2 Génie Informatique \\ Matricule : 21P474 \\ ENSPY}
\date{22 Septembre 2025}

\begin{document}

\maketitle

\section{Partie 1: Fondations du Deep Learning}

\subsection*{Question 1: Couches Dense, Dropout et Softmax}

La couche \textbf{Dense (Fully Connected)} est la couche fondamentale où chaque neurone est connecté à chaque neurone de la couche précédente, permettant au modèle d'apprendre des relations complexes entre les caractéristiques d'entrée. Elle applique une transformation linéaire suivie d'une activation (ReLU).

La couche \textbf{Dropout} est une technique de régularisation qui consiste à désactiver aléatoirement un certain pourcentage de neurones (ici, $20\%$) pendant l'entraînement. Son rôle est de prévenir le \textbf{surapprentissage (overfitting)} en forçant le réseau à ne pas dépendre de neurones spécifiques.

La fonction d'activation \textbf{Softmax} est utilisée dans la couche de sortie pour la \textbf{classification multi-classes} (10 classes). Elle transforme les valeurs numériques (logits) en une distribution de probabilités, où la classe ayant la probabilité la plus élevée est la prédiction finale du modèle.

\subsection*{Question 2: Optimiseur Adam vs SGD}

L'optimiseur \textbf{Adam (Adaptive Moment Estimation)} est préféré à la \textbf{Descente de Gradient Stochastique simple (SGD)} car il adapte dynamiquement le taux d'apprentissage pour chaque paramètre du modèle.

Adam combine deux mécanismes majeurs : le \textbf{Momentum} (qui utilise une moyenne mobile des gradients passés) et \textbf{RMSprop} (qui adapte le taux d'apprentissage en fonction de la magnitude des gradients). Cette adaptation permet une convergence plus rapide et plus stable dans la majorité des architectures de Deep Learning.

\subsection*{Question 3: Vectorisation et calcul par lots}

Ces deux concepts sont essentiels pour exploiter l'efficacité des calculs sur des architectures parallèles (CPU/GPU) :
\begin{itemize}
    \item \textbf{Vectorisation :} C'est la capacité d'effectuer des opérations sur des tableaux entiers (tenseurs) sans boucles explicites. Les opérations de normalisation des données et les produits matriciels effectués par les couches \texttt{Dense} sont vectorisés grâce à TensorFlow/NumPy.
    \item \textbf{Calcul par lots (Batch Processing) :} La division des données d'entraînement en sous-ensembles de taille fixe (\texttt{batch\_size=128}) permet d'optimiser l'utilisation de la mémoire et d'accélérer la vitesse d'entraînement grâce à un parallélisme accru. La mise à jour des poids n'est effectuée qu'après le traitement de chaque lot.
\end{itemize}

\section{Partie 2: Ingénierie du Deep Learning}

\subsection*{Suivi MLflow}

Le suivi des expérimentations a été mis en place en utilisant \textbf{MLflow}, ce qui est indispensable pour une gestion professionnelle des modèles. Chaque exécution (\emph{run}) du script \texttt{train.py} a été enregistrée.

Pour l'exécution de base, nous avons obtenu une précision de \textbf{< Précision obtenue >} sur le jeu de test après 5 epochs. L'outil a permis de :
\begin{enumerate}
    \item \textbf{Tracer les Hyperparamètres} (\texttt{epochs=5}, \texttt{batch\_size=128}, \texttt{dropout\_rate=0.2}).
    \item \textbf{Enregistrer les Métriques} (\texttt{test\_accuracy} et \texttt{test\_loss}).
    \item \textbf{Archiver le Modèle} : Le modèle Keras entraîné est sérialisé et stocké sous forme d'artefact pour la phase de déploiement.
\end{enumerate}

\subsection*{Déploiement et CI/CD}

\subsubsection*{Question 1: Pipeline CI/CD}

Un pipeline de CI/CD (Intégration et Livraison Continues) automatisé via \textbf{GitHub Actions} permet de transformer le code en service déployé sans intervention manuelle après le commit. Les étapes sont :
\begin{enumerate}
    \item \textbf{Build (Construction) :} Le \texttt{Dockerfile} est utilisé pour créer l'image Docker, qui est ensuite taguée et poussée vers un registre de conteneurs (ex: Google Artifact Registry).
    \item \textbf{Deploy (Déploiement) :} L'action se connecte au fournisseur Cloud (ex: Google Cloud Run) et déploie le conteneur mis à jour sur le service, assurant une mise à jour fiable et rapide de l'API d'inférence.
\end{enumerate}

\subsubsection*{Question 2: Monitoring en production}

Trois types d'indicateurs clés (KPIs) essentiels pour le monitoring de l'API d'inférence en production sont :

\begin{enumerate}
    \item \textbf{Indicateurs de Performance (Technique) :} \textbf{Latence de l'API} (temps de réponse aux requêtes \texttt{/predict}) et \textbf{Taux d'erreurs} (HTTP 5xx).
    \item \textbf{Indicateurs de Modèle (Qualité) :} \textbf{Drift des données (Data Drift)} (changement de la distribution des données d'entrée) et \textbf{Distribution des Prédictions} (pour détecter un \emph{Model Drift}).
    \item \textbf{Indicateurs Métiers/Opérationnels :} \textbf{Throughput (Débit)} (requêtes par seconde) pour la gestion de la charge et le dimensionnement des ressources.
\end{enumerate}


\section*{Conclusion}
Ce TP a permis d'appliquer les principes d'ingénierie logicielle (structuration, versionnement, conteneurisation) au développement d'un modèle de Deep Learning. L'utilisation de MLflow a introduit les bonnes pratiques de suivi des expérimentations, et la conteneurisation via Docker a préparé le modèle pour un déploiement robuste en environnement de production, constituant une base solide pour la suite du cursus.

\section*{Référence au Code Source}
Conformément aux bonnes pratiques de l'ingénierie logicielle, le code source complet (scripts d'entraînement, API Flask et Dockerfile) est versionné et disponible sur GitHub à l'adresse suivante :

\url{https://github.com/ThePerformer0/deep-learning-engineering-labs/tree/main/tp1-mnist-lifecycle}

\end{document}